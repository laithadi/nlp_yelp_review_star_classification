{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ba65e3-f3bb-4b56-8872-c4ad3ff8cf4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4707758-c9c9-4260-80fa-cfe5d9d47f01",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -Iv transformers==4.38.1\n",
    "# !pip install -Iv datasets==2.1.0\n",
    "# !pip install -Iv tensorflow==2.15.0\n",
    "# !pip install -Iv keras==3.0.5\n",
    "# !pip install tf-keras\n",
    "# !pip install -Iv nltk==3.2.4\n",
    "# !pip install -Iv contractions==0.1.73\n",
    "# !pip install -Iv accelerate==0.27.2\n",
    "# !pip install -Iv scikit-learn==1.4.0\n",
    "\n",
    "# !pip install datasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758e973f-ccfb-4a53-ab93-a01784b8cfcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 00:56:06.306480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, load_dataset, load_metric\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding, DistilBertConfig, DistilBertForSequenceClassification, BertConfig, BertForSequenceClassification, create_optimizer, TrainingArguments, Trainer\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import pipeline\n",
    "import csv\n",
    "import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e15d91-ef49-4c15-935c-076bf7ca698f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8751bfa4-2c0b-4241-8bb8-b97279565749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_path = 'data/train.csv'\n",
    "test_data_path = 'data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe48ba27-0e35-4662-9775-060001a02db0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WILL NEVER COME BACK! HORRIBLE SERVICE &amp; NASTY...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible food.  Terrible service.\\n\\nThe absol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So, right away if I go into a buffet setting, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have gotten good cuts from this place. I eve...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I felt this place was a bit lackluster conside...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  WILL NEVER COME BACK! HORRIBLE SERVICE & NASTY...      1\n",
       "1  Terrible food.  Terrible service.\\n\\nThe absol...      1\n",
       "2  So, right away if I go into a buffet setting, ...      2\n",
       "3  I have gotten good cuts from this place. I eve...      3\n",
       "4  I felt this place was a bit lackluster conside...      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data = pd.read_csv(train_data_path)\n",
    "labelled_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5220cd1d-8170-4f0e-b85e-f852a0c50f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labelled_data['stars'] = labelled_data['stars'].replace({1: 'one_star', 2: 'two_stars', 3: 'three_stars'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fdd68cf-a963-4152-afb1-94e59f797be1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WILL NEVER COME BACK! HORRIBLE SERVICE &amp; NASTY...</td>\n",
       "      <td>one_star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible food.  Terrible service.\\n\\nThe absol...</td>\n",
       "      <td>one_star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So, right away if I go into a buffet setting, ...</td>\n",
       "      <td>two_stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have gotten good cuts from this place. I eve...</td>\n",
       "      <td>three_stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I felt this place was a bit lackluster conside...</td>\n",
       "      <td>two_stars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        stars\n",
       "0  WILL NEVER COME BACK! HORRIBLE SERVICE & NASTY...     one_star\n",
       "1  Terrible food.  Terrible service.\\n\\nThe absol...     one_star\n",
       "2  So, right away if I go into a buffet setting, ...    two_stars\n",
       "3  I have gotten good cuts from this place. I eve...  three_stars\n",
       "4  I felt this place was a bit lackluster conside...    two_stars"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb96955-9796-4242-b63d-bf773d3a56c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116bdd58-7a02-498b-bee2-289e7c4a32cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>one_star</th>\n",
       "      <th>three_stars</th>\n",
       "      <th>two_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WILL NEVER COME BACK! HORRIBLE SERVICE &amp; NASTY...</td>\n",
       "      <td>one_star</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible food.  Terrible service.\\n\\nThe absol...</td>\n",
       "      <td>one_star</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So, right away if I go into a buffet setting, ...</td>\n",
       "      <td>two_stars</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have gotten good cuts from this place. I eve...</td>\n",
       "      <td>three_stars</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I felt this place was a bit lackluster conside...</td>\n",
       "      <td>two_stars</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Back when I lived in Pollock Halls in my first...</td>\n",
       "      <td>three_stars</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Short version:\\n- 20 stars for having free vod...</td>\n",
       "      <td>two_stars</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Okay.....read the other reviews about taking t...</td>\n",
       "      <td>one_star</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The food is really good, but we had one big co...</td>\n",
       "      <td>three_stars</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cool place, comfortable. Sorely needed in the ...</td>\n",
       "      <td>three_stars</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        stars  one_star  \\\n",
       "0  WILL NEVER COME BACK! HORRIBLE SERVICE & NASTY...     one_star      True   \n",
       "1  Terrible food.  Terrible service.\\n\\nThe absol...     one_star      True   \n",
       "2  So, right away if I go into a buffet setting, ...    two_stars     False   \n",
       "3  I have gotten good cuts from this place. I eve...  three_stars     False   \n",
       "4  I felt this place was a bit lackluster conside...    two_stars     False   \n",
       "5  Back when I lived in Pollock Halls in my first...  three_stars     False   \n",
       "6  Short version:\\n- 20 stars for having free vod...    two_stars     False   \n",
       "7  Okay.....read the other reviews about taking t...     one_star      True   \n",
       "8  The food is really good, but we had one big co...  three_stars     False   \n",
       "9  Cool place, comfortable. Sorely needed in the ...  three_stars     False   \n",
       "\n",
       "   three_stars  two_stars  \n",
       "0        False      False  \n",
       "1        False      False  \n",
       "2        False       True  \n",
       "3         True      False  \n",
       "4        False       True  \n",
       "5         True      False  \n",
       "6        False       True  \n",
       "7        False      False  \n",
       "8         True      False  \n",
       "9         True      False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode the labels \n",
    "one_hot_labels = pd.get_dummies(labelled_data['stars'])\n",
    "# # drop the old 'stars' column to replace with the one hot encoded values \n",
    "# labelled_data = labelled_data.drop('stars', axis= 1)\n",
    "# join with the one hot encoded \n",
    "labelled_data = labelled_data.join(one_hot_labels)\n",
    "\n",
    "labelled_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635b2c15-b336-4f75-b7ca-ad54e3cae706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 1. Expand Contractions using contractions library\n",
    "# def expand_contractions(text):\n",
    "#     return contractions.fix(text)\n",
    "\n",
    "# # Apply expand_contractions function to 'text' column\n",
    "# labelled_data['text'] = labelled_data['text'].apply(expand_contractions)\n",
    "\n",
    "# # 2. Lower Case\n",
    "# labelled_data['text'] = labelled_data['text'].str.lower()\n",
    "\n",
    "# # 3. Remove Punctuations\n",
    "# labelled_data['text'] = labelled_data['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# # 4. Remove Extra Spaces\n",
    "# labelled_data['text'] = labelled_data['text'].apply(lambda x: re.sub(' +', ' ', x.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9d3075a-a13b-4f3b-b235-e46a7fb37aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Download the WordNet resource\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870227d5-4752-4a64-b8af-575cdb0d7ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 5. Remove Stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# labelled_data['text'] = labelled_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb937415-a715-46b8-891e-dfa040bf22dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Download the WordNet resource\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56e743eb-301c-4d39-ad4b-5f308fe22fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !unzip /home/ec2-user/nltk_data/corpora/wordnet.zip -d /home/ec2-user/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "261ab0ae-371b-44ee-8693-5364e797e647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 6. Stemming and Lemmatization\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# def lemmatize_words(text):\n",
    "#     return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "# labelled_data['text'] = labelled_data['text'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f803d682-0f1d-44e9-a6bb-78356e51285c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# labelled_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35d50d-9fc7-4c8e-9a9f-7eb4dba25589",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01886e06-9be0-4441-a338-c2d953c65e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    labelled_data['text'], \n",
    "    labelled_data[['one_star', 'two_stars', 'three_stars']], \n",
    "    test_size= 0.001, \n",
    "    random_state= 42, \n",
    "    stratify= labelled_data['stars']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13cff12f-028b-4840-90e1-fd090ca82fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_star  two_stars  three_stars\n",
      "False     False      True           1998\n",
      "          True       False          1998\n",
      "True      False      False          1998\n",
      "Name: count, dtype: int64\n",
      "one_star  two_stars  three_stars\n",
      "False     False      True           2\n",
      "          True       False          2\n",
      "True      False      False          2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_valid.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2369ed47-2ac0-472b-a568-ca868d9b255d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1882e3a4-17f7-4151-88f9-0c4559823309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0463147-4173-42ee-b13e-81491c1f0e14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5994\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "train_ind = X_train.index.values.tolist()\n",
    "valid_ind = X_valid.index.values.tolist()\n",
    "\n",
    "train_data = {'text': [], 'one_star': [], 'two_stars': [], 'three_stars': []}\n",
    "valid_data = {'text': [], 'one_star': [], 'two_stars': [], 'three_stars': []}\n",
    "\n",
    "for ti in train_ind:\n",
    "    train_data['text'].append(X_train[ti])\n",
    "    train_data['one_star'].append(y_train.loc[ti]['one_star'])\n",
    "    train_data['two_stars'].append(y_train.loc[ti]['two_stars'])\n",
    "    train_data['three_stars'].append(y_train.loc[ti]['three_stars'])\n",
    "\n",
    "for vi in valid_ind:\n",
    "    valid_data['text'].append(X_valid[vi])\n",
    "    valid_data['one_star'].append(y_valid.loc[vi]['one_star'])\n",
    "    valid_data['two_stars'].append(y_valid.loc[vi]['two_stars'])\n",
    "    valid_data['three_stars'].append(y_valid.loc[vi]['three_stars'])\n",
    "    \n",
    "print(len(train_data['text']))\n",
    "print(len(valid_data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "821db20f-0478-4d9f-8acb-cfc838adf6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(train_data)\n",
    "valid_dataset = Dataset.from_dict(valid_data)\n",
    "data = datasets.DatasetDict({\"train\": train_dataset,\"valid\": valid_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0601ffdd-49af-4e7a-8c60-dabdcd91b7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'one_star', 'two_stars', 'three_stars'],\n",
       "        num_rows: 5994\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'one_star', 'two_stars', 'three_stars'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85de4c8d-0fe5-4e70-893a-c491e05815b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"I live down the street from this place and wish for nothing more than for it to be better.  I was excited to hear the items on the menu.  I am a sucker for a sandwich with lots of abstract foods on them.  I have tried The Student and The Professor on multiple occasions.  Every time I just wish they had a different sauce on them and more of it.  The sandwiches are dry and lacking in flavor.  If you ask them to put more sauce on (which I did after the first time), they charge you extra.  I dig the creativity but it is lacking in everything else.  \\\\n\\\\nThe wait is also ridiculous.  It takes almost 20 mins each time I've been there for them to make the sandwich even if there's no one else there.  Not to mention the people who work there are rude.\",\n",
       " 'one_star': False,\n",
       " 'two_stars': True,\n",
       " 'three_stars': False}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afbff5ac-4622-4930-8a74-c6c170ea3c85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one_star', 'two_stars', 'three_stars']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in data['train'].features.keys() if label not in ['text']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1cb89db-2f79-4127-8f02-d64b58c3226a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'one_star', 1: 'two_stars', 2: 'three_stars'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b994731-ad04-453c-a258-2c1bee196655",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3aa05bba-430c-4cb9-b93f-d0a23f4bd48c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3778f82f8c40618c0ef9fc6c61e3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804e3077a0a94c6d8f8ea4c1d0ac95d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376ed9bc4dd64649945761e5508675a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1020cdd9ee5c4777a4372a34a0514fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"text\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "445d7735-b1b0-4d61-a666-c55f816f20d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960297ba5b5b4ff197f6c51af4c6742b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5994 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ac81fa8eea43c38604f6061054666a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data.map(preprocess_data, batched=True, remove_columns= data['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36d745c4-5223-40ed-80fb-d6d2a73f7ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5994\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb7faa7e-1f8a-4faf-b1ec-d80204005f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 0.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad602c24-ebfe-4f39-8030-0ef1f8695bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_data.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83c00b-3afe-40c9-9222-13c413a0aaad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80ccc166-6765-42b8-90dc-e3b67232407a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9ee71-459f-4653-a2b1-c9377ab2eb9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd9be9bd-1cc2-45ae-833e-b02d6040b332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042c0a91095f4af096917bde779e54a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-base\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7f671-413a-4afe-937d-ddcfc90cfa9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train Model Pt.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efd53ef9-3a92-4a4b-b7f4-a733c4dbade6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e774d9cb-5d67-417f-92fb-3c868435ffc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70132ca4-4734-460d-8fb6-c55de90f40a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"bert-model-stat940-roberto\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-9, # 2e-5\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24fe3076-1618-49ef-a181-502ce90d83d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7faef-ed2c-4a31-82cc-0f75f1bf3e9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Verify a batch and forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39c23668-d6ca-4ba0-b54b-fcd2eaee7ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1eff2ab-f1a7-48e8-b63f-5aeb4af62970",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   100,   697,   159,     5,  2014,    31,    42,   317,     8,\n",
       "         2813,    13,  1085,    55,    87,    13,    24,     7,    28,   357,\n",
       "            4,  1437,    38,    21,  2283,     7,  1798,     5,  1964,    15,\n",
       "            5,  5765,     4,  1437,    38,   524,    10, 40454,    13,    10,\n",
       "        15649,    19,  3739,     9, 20372,  6592,    15,   106,     4,  1437,\n",
       "           38,    33,  1381,    20,  9067,     8,    20,  6020,    15,  1533,\n",
       "         7657,     4,  1437,  4337,    86,    38,    95,  2813,    51,    56,\n",
       "           10,   430,  8929,    15,   106,     8,    55,     9,    24,     4,\n",
       "         1437,    20, 19072,    32,  3841,     8, 12622,    11, 12117,     4,\n",
       "         1437,   318,    47,  1394,   106,     7,   342,    55,  8929,    15,\n",
       "           36,  5488,    38,   222,    71,     5,    78,    86,   238,    51,\n",
       "         1427,    47,  1823,     4,  1437,    38,  8512,     5, 11140,    53,\n",
       "           24,    16, 12622,    11,   960,  1493,     4,     2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d1c9388-bd66-4cb6-b38d-e9b40d9c2d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqSequenceClassifierOutput(loss=tensor(0.6300, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.0686,  0.2814, -0.0505]], grad_fn=<AddmmBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.0333,  0.0095, -0.0021,  ...,  0.0112, -0.0015, -0.0075],\n",
       "         [ 0.1670,  0.3363,  0.0336,  ..., -0.0555, -0.2182,  0.4453],\n",
       "         [-0.0161,  0.2885,  0.0080,  ...,  0.0285,  0.0655, -0.1562],\n",
       "         ...,\n",
       "         [ 0.0125, -0.2285, -0.0881,  ..., -0.1711, -0.6356,  0.4325],\n",
       "         [ 0.0657,  0.0190,  0.1704,  ...,  0.0559, -0.2105,  0.1363],\n",
       "         [ 0.0725,  0.0399,  0.2782,  ...,  0.0220, -0.1229,  0.3979]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(input_ids=tokenized_data['train']['input_ids'][0].unsqueeze(0), labels=tokenized_data['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf8f1a2-02f1-4f94-9738-6457a4b8fe71",
   "metadata": {},
   "source": [
    "# # Train Model Pt.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbc9637f-1f80-4540-86d0-269a54ebf788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca287969-065a-4703-aa9e-ec529bbacfc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1880' max='1880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1880/1880 17:14, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192357</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192342</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.404700</td>\n",
       "      <td>0.192345</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.404700</td>\n",
       "      <td>0.192340</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.404700</td>\n",
       "      <td>0.192342</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.192341</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.192336</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.192336</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.192337</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.192337</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory bert-model-stat940-roberto/checkpoint-188 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Checkpoint destination directory bert-model-stat940-roberto/checkpoint-376 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Checkpoint destination directory bert-model-stat940-roberto/checkpoint-564 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Checkpoint destination directory bert-model-stat940-roberto/checkpoint-752 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Checkpoint destination directory bert-model-stat940-roberto/checkpoint-940 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Checkpoint destination directory bert-model-stat940-roberto/checkpoint-1128 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Checkpoint destination directory bert-model-stat940-roberto/checkpoint-1316 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Checkpoint destination directory bert-model-stat940-roberto/checkpoint-1504 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Checkpoint destination directory bert-model-stat940-roberto/checkpoint-1692 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Checkpoint destination directory bert-model-stat940-roberto/checkpoint-1880 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1880, training_loss=0.4220225313876538, metrics={'train_runtime': 1035.4139, 'train_samples_per_second': 57.89, 'train_steps_per_second': 1.816, 'total_flos': 4595743878497280.0, 'train_loss': 0.4220225313876538, 'epoch': 10.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41437429-1723-46b4-b6a3-90b0e7b16c55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe7b1df-05b1-4208-a831-568c7bbdd86f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.save_pretrained('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74dff90f-b88e-44b7-95c9-79f955a2b7ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7568681240081787,\n",
       " 'eval_f1': 0.6399331662489557,\n",
       " 'eval_roc_auc': 0.7299999999999999,\n",
       " 'eval_accuracy': 0.635}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7157b-ac45-4633-99dc-a3a755e288df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e1ad3b6-1819-4b24-820f-c3d48979aec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\nDuring my recent company trip to our Tempe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n\\nAfter hearing about Pauly D from Jersey Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\\n\\nI had high hopes for this restaurant based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>We experienced overpriced and underwhelming en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\\n\\nThe windows from this company are definite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               text\n",
       "0   1  \\n\\nDuring my recent company trip to our Tempe...\n",
       "1   2  \\n\\nAfter hearing about Pauly D from Jersey Sh...\n",
       "2   3  \\n\\nI had high hopes for this restaurant based...\n",
       "3   4  We experienced overpriced and underwhelming en...\n",
       "4   5  \\n\\nThe windows from this company are definite..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(test_data_path)\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6cb1338b-8e44-4053-9e01-a90006b17d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = 'bart_base_v03.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59412f3f-6eac-4512-ba5c-07fcb33a34ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "error_ind = []\n",
    "with open(output_path, \"w\", newline='') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow([\"ID\",\"Label\"])\n",
    "    for index, row in test_data.iterrows():\n",
    "        text_id = row['ID']\n",
    "        text = row['text']\n",
    "        encoding = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "        # print(encoding)\n",
    "        # print(encoding['attention_mask'].size())\n",
    "        try:\n",
    "            outputs = trainer.model(**encoding)\n",
    "        except: \n",
    "            count += 1\n",
    "            error_ind.append(text_id)\n",
    "        logits = outputs.logits\n",
    "        # apply sigmoid + threshold\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        probs = sigmoid(logits.squeeze().cpu())\n",
    "        predictions = np.zeros(probs.shape)\n",
    "        predictions[np.argmax(probs.detach().numpy())] = 1\n",
    "        # turn predicted id's into actual label names\n",
    "        predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "        \n",
    "        if predicted_labels[0] == 'one_star':\n",
    "            conv_pred = 1\n",
    "        elif predicted_labels[0] == 'two_stars':\n",
    "            conv_pred = 2\n",
    "        else: \n",
    "            conv_pred = 3\n",
    "\n",
    "        \n",
    "        csv_writer.writerow([text_id,conv_pred])\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530e8a4-aaec-40f9-afde-ae0c8ff5dad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291ae4a-f995-4683-a662-a77a7bf7ea1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
